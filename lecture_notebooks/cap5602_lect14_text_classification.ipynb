{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lect14-text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63GSckRspoZJ"
      },
      "source": [
        "# Text classification (CAP5602 Lecture 14)\n",
        "\n",
        "In this demo, we will do text classification with the [20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). We will adapt the tutorial here from sklearn:\n",
        "*   [https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAF5AZsAIol"
      },
      "source": [
        "## 1. Download and load data\n",
        "\n",
        "We use the [fetch_20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) method from sklearn to download and load the data into memory. Here we only use 3 classes (*rec.motorcycles*, *comp.graphics*, and *sci.med*) for our experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh3As9Ey0Aqz"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['rec.motorcycles', 'comp.graphics', 'sci.med']\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u-lZbz3Auf4"
      },
      "source": [
        "## 2. Print an example and its label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hef3TlAbAxff"
      },
      "source": [
        "id = 6\n",
        "label = twenty_train.target[id]\n",
        "\n",
        "print(twenty_train.data[id]) # Print the input text\n",
        "print(twenty_train.target_names[label]) # Print the label name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lql4o8WQCCXX"
      },
      "source": [
        "## 3. Count the n-gram tokens\n",
        "\n",
        "Next, we use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class to do pre-processing, tokenizing, and counting the n-gram tokens altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN2ja5ci2n0B"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object that counts unigrams and bigrams\n",
        "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Count (Fit) the features from train data and also transform the data into count vectors\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "\n",
        "print(X_train_counts.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Zu5yuCDf0f"
      },
      "source": [
        "## 4. Convert count matrix to Tf-idf matrix\n",
        "\n",
        "To do this, we use the [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) with the default parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qk61QCE2CHO"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMyEqT0lHNiM"
      },
      "source": [
        "## 5. Train a classifier\n",
        "\n",
        "Now we can train a classifier as usual with the Tf-idf matrix. Here we will use the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NImA7tOY21ef"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ7jgpjZHb1j"
      },
      "source": [
        "## 6. Evaluate the trained classifier\n",
        "\n",
        "To evaluate our trained classifier, we will fetch the test dataset and transform them into Tf-idf matrix using `count_vect` and `tfidf_transformer` above. Note that during test time, we do not fit these objects again. Then we make predictions using the Tf-idf matrix and compute the accuracy as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cKJ1rCQHjdk"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fetch test data\n",
        "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "# Transform test data into count matrix and then Tf-idf matrix\n",
        "X_test_counts = count_vect.transform(twenty_test.data)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Make predictions on the Tf-idf matrix and compute accuracy\n",
        "Y_pred = model.predict(X_test_tfidf)\n",
        "acc = accuracy_score(twenty_test.target, Y_pred)\n",
        "print('Accuracy on test set:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbcaMjD3I4dR"
      },
      "source": [
        "## 7. Using a pipeline\n",
        "\n",
        "Sklearn allows us to create a pipeline to combine all the processing steps (counting, transforming to Tf-idf, and classifying)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uEQVhJvf3Q1s"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline object to combine the processing steps, you can choose your own name for each step\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LogisticRegression()),\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "text_clf.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "# Predict with the baseline\n",
        "Y_pred = text_clf.predict(twenty_test.data)\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(twenty_test.target, Y_pred)\n",
        "print('Accuracy on test set:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}